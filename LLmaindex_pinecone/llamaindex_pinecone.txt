import os
import json
import time
import logging
from logging.handlers import RotatingFileHandler
from typing import List

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# --------------------------
# LOGGING (FILE + CONSOLE)
# --------------------------

if not os.path.exists("logs"):
    os.makedirs("logs")

formatter = logging.Formatter(
    "%(asctime)s | %(levelname)s | %(message)s"
)

file_handler = RotatingFileHandler(
    "logs/rag_fast.log",
    maxBytes=5_000_000,
    backupCount=3,
    encoding="utf-8"
)
file_handler.setFormatter(formatter)
file_handler.setLevel(logging.INFO)

console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
console_handler.setLevel(logging.INFO)

log = logging.getLogger("rag-fast")
log.setLevel(logging.INFO)
log.addHandler(file_handler)
log.addHandler(console_handler)


# --------------------------
# FASTAPI SETUP
# --------------------------

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# --------------------------
# LLAMAINDEX + PINECONE
# --------------------------

from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    Settings
)
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.core.node_parser import SimpleNodeParser

from pinecone import Pinecone, ServerlessSpec


# ---- API keys ----
GEMINI_API_KEY = ""
PINECONE_API_KEY = ""
INDEX_NAME = "quiz-index"


# Global index
index = None


# --------------------------
# MODELS
# --------------------------

class QuizRequest(BaseModel):
    topic: str
    difficulty: str

class Answer(BaseModel):
    selected: str
    correct: str

class ScoreRequest(BaseModel):
    answers: List[Answer]


# --------------------------
# CLEAN JSON
# --------------------------

def clean_json(txt):
    txt = txt.replace("```json", "").replace("```", "").strip()
    try:
        return json.loads(txt)
    except:
        if "[" in txt and "]" in txt:
            try:
                sub = txt[txt.index("["): txt.rindex("]") + 1]
                return json.loads(sub)
            except:
                pass
        return None


# --------------------------
# FAST LLM CALL
# --------------------------

async def llm_call_fast(prompt: str):
    try:
        start = time.perf_counter()
        res = await Settings.llm.acomplete(prompt)
        duration = round(time.perf_counter() - start, 4)
        log.info(f"‚ö° FAST LLM Time: {duration} sec")
        return res.text

    except Exception:
        log.error("LLM call error:", exc_info=True)
        raise


# --------------------------
# STARTUP ‚Üí BUILD INDEX
# --------------------------

@app.on_event("startup")
async def startup_event():
    global index

    try:
        log.info("üìÇ Loading documents from /pdfs ...")

        if not os.path.exists("pdfs"):
            os.makedirs("pdfs")

        docs = SimpleDirectoryReader("pdfs").load_data()
        log.info(f"üìÑ Loaded {len(docs)} docs")

        # Node splitter
        parser = SimpleNodeParser.from_defaults(
            chunk_size=1024,
            chunk_overlap=100
        )
        nodes = parser.get_nodes_from_documents(docs)
        log.info(f"üß© Chunked to {len(nodes)} nodes")

        # Configure global Settings for LlamaIndex
        Settings.llm = Gemini(
            model="gemini-2.5-flash",
            api_key=GEMINI_API_KEY
        )
        Settings.embed_model = GeminiEmbedding(
            model="models/text-embedding-004",
            api_key=GEMINI_API_KEY
        )

        # Init Pinecone
        pc = Pinecone(api_key=PINECONE_API_KEY)

        # Create index if missing
        if INDEX_NAME not in pc.list_indexes().names():
            log.info("üõ† Creating Pinecone index ...")
            pc.create_index(
                name=INDEX_NAME,
                dimension=768,
                metric="cosine",
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"
                )
            )

        pinecone_index = pc.Index(INDEX_NAME)

        # PINECONE vector store
        vector_store = PineconeVectorStore(pinecone_index=pinecone_index)

        # Storage context (THIS is required for saving vectors)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store
        )

        # Build vector index (this pushes embeddings into Pinecone)
        start_vec = time.perf_counter()
        index = VectorStoreIndex(
            nodes,
            storage_context=storage_context
        )
        duration = round(time.perf_counter() - start_vec, 4)

        # Explicit persist push
        index.storage_context.persist()

        log.info(f"‚ú® Stored {len(nodes)} vectors in Pinecone")
        log.info(f"üîê Pinecone index ready in {duration} sec")
        log.info("üöÄ FAST RAG Engine Ready (LlamaIndex + Pinecone)")

    except Exception:
        log.error("Startup error:", exc_info=True)
        raise


# --------------------------
# GET TOPICS
# --------------------------

@app.get("/topics")
async def get_topics():
    prompt = """
    Return a JSON array of chemistry topics.
    Example: ["Organic Chemistry", "Inorganic Chemistry"]
    """
    raw = await llm_call_fast(prompt)
    data = clean_json(raw)

    if data:
        return {"topics": data}

    return {"topics": ["Fallback Topic"]}


# --------------------------
# GENERATE QUIZ
# --------------------------

@app.post("/start_quiz")
async def start_quiz(req: QuizRequest):
    global index

    try:
        retriever = index.as_retriever(similarity_top_k=5)

        start_ret = time.perf_counter()
        nodes = retriever.retrieve(req.topic)
        retrieval_time = round(time.perf_counter() - start_ret, 4)

        log.info(f"üîé RAG Retrieval Time: {retrieval_time} sec")
        log.info(f"üìö Retrieved {len(nodes)} chunks")

    except Exception:
        nodes = []
        retrieval_time = None

    # Build context
    context = "\n\n".join([n.get_content() for n in nodes])

    prompt = f"""
    Generate 10 MCQ questions in JSON format.

    Topic: "{req.topic}"
    Difficulty: "{req.difficulty}"

    Context:
    {context}

    Format:
    [
        {{"q": "...", "A": "...", "B": "...", "C": "...", "D": "...", "correct": "A"}}
    ]
    """

    raw = await llm_call_fast(prompt)
    quiz = clean_json(raw)

    if quiz and len(quiz) == 10:
        return {
            "quiz": quiz,
            "retrieval_time": retrieval_time
        }

    raise HTTPException(status_code=500, detail="Invalid quiz JSON")


# --------------------------
# SCORE
# --------------------------

@app.post("/final_score")
async def final_score(req: ScoreRequest):
    score = sum(1 for x in req.answers if x.selected == x.correct)
    return {"score": score, "total": len(req.answers)}